FMW Database Schemas Export and Import Scripts  
Copyright (c) 2025 Oracle and/or its affiliates  
Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/  

# FMW Database Schemas Export and Import Scripts
These scripts are designed to export and import a FMW database metadata repository from a source database to a target database. They can be used, also, to move to a different PDB in the same CDB. This method can be used as a "backup and restore" procedure for the schemas used by FMW domains (each domain is characterized by a precise database schema prefix, also known as "RCU PREFIX").

| Script name  | Description |
| ------------- | ------------- |
| [export_fmw.sh](./export_fmw.sh) | This script exports the tablespaces, schemas and roles of an Oracle FMW domain. It uses Oracle Data Pump Export and DDL statements to extract the information from the PDB used by a precise FMW system. It creates an output tar that can be transferred to another database system to migrate the original FMW domain. |
| [import_fmw.sh](./import_fmw.sh) | This script imports the tablespaces, schemas and roles of an Oracle FMW domain. It uses Oracle Data Pump Import and DDL statements to import the output generated by the [export_fmw.sh](./export_fmw.sh) script to a different PDB or to an entirely different database. |

### Features
- The scripts use **Oracle Data Pump** and DDL statements to export/import the information.
- The scripts export all the schemas, tablespaces, roles and permissions of the schemas that start with the **RCU PREFIX** provided as input.
- The scripts also export and import the FMW metadata information that resides in the SYSTEM schema (the **schema_registry** objects) for the RCU PREFIX provided as input.
- The import will NOT overwrite other RCU prefixes existing in the same PDB's SYSTEM schema. It will only add rows for the prefix included in the export.

### Requirements
- The scripts must be executed in the source (export) and target (import) database nodes.
- A staging directory is required in both the source and target database nodes with enough space to store the exported data.
- Both the export and import scripts require a TNS alias to connect to the precise PDB. This TNS alias must map to a service that is attached to a SINGLE instance (i.e. not to al the instances in a RAC database configuration). This is done to avoid requiring external shared storage in the DB nodes. You must create this instance-specific service and an alias for it in the database's tnsnames.ora file. If the amount of data in FMW schemas is large (anything beyond 100-200 GB) it is recommended to configure shared storage and set DatPump paralellism for increased perfromance. Refer to the [Oracle Data Pump Performance recommendations](https://docs.oracle.com/en/database/oracle/oracle-database/23/sutil/oracle-data-pump-performance-tips.html#GUID-1AD6C874-13C2-4F5F-A83E-0848A789D786) for details.

Here is an example for the creation of a service atttached to a single instance in a RAC database (notice the "preferred" option where only one of the DB instances is used):  
  `[oracle@fmwdbnode1 ~]$ srvctl add service -db $ORACLE_UNQNAME -service export_soaedg.example.com -preferred SOADB1 -pdb SOADB_pdb1`  
  `[oracle@fmwdbnode1 ~]$ srvctl start service -s  export_soaedg.example.com -db $ORACLE_UNQNAME`  
  `[oracle@fmwdbnode1 ~]$ lsnrctl status | grep export_soaedg.example.com`  
  `Service "export_soaedg.example.com" has 1 instance(s).`  
  `[oracle@fmwdbnode1 ~]$ cat /u01/app/oracle/product/23.0.0.0/dbhome_1/network/admin/tnsnames.ora | grep export`  
  `EXPORT_SOADB_PDB1=(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=dbexample-scan.dbsubnet.vcnexample.oraclevcn.com)(PORT=1521))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=export_soaedg.example.com)(FAILOVER_MODE=(TYPE=select)(METHOD=basic))))`  

### Considerations
- Schemas, tablespaces, and users that are not part of the FMW metadata schemas are not included in the export and import.
- To guarantee consistency, the AQ queues used by the FMW schemas are stopped in the source database during the export operation. They are started again once the export operation concludes.
